---
title: "homework-number-5"
output: html_document
---

---
title: "homework-number-5"
output: html_document
---
```{r}
knitr::opts_chunk$set(
    echo = TRUE,
    warning = FALSE,
    comment = "##",
    prompt = TRUE,
    tidy = TRUE,
    tidy.opts = list(width.cutoff = 75),
    fig.path = "img/"
)
```

##Bootstrapping Standard Errors and CIs for Linear Models.

###When we initially discussed the central limit theorem and confidence intervals, we showed how we could use bootstrapping to estimate standard errors and confidence intervals around certain parameter values, like the mean. Using bootstrapping, we could also do the same for estimating standard errors and CIs around regression parameters, such as $\beta$ coefficients.

###Using the "KamilarAndCooperData.csv" dataset, run a linear regression looking at log(HomeRange_km2) in relation to log(Body_mass_female_mean) and report your $\beta$ coeffiecients (slope and intercept).

###Then, use bootstrapping to sample from your data 1000 times with replacement, each time fitting the same model and calculating the same coefficients. This generates a sampling distribution for each parameter.

###Estimate the standard error for each of your $\beta$ coefficients as the standard deviation of the sampling distribution and determine the 95% CI for each of your $\beta$ coefficients based on the appropriate quantiles from your sampling distribution. How does the former compare to the SE estimated from your entire dataset using the formula for standard error? How does the latter compare to the 95% CI estimated from your dataset?

```{r}
#read in data
install.packages("dplyr")
library(dplyr)
library(curl)
f <- curl("https://raw.githubusercontent.com/difiore/ADA2016/master/KamilarAndCooperData.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = TRUE)
head(d)
plot(data = d, HomeRange_km2 ~ Body_mass_female_mean)
```

##linear regression looking at logs of data
```{r}

library(ggplot2)
logg <- ggplot(mod, aes(x = bm, y = hr)) + geom_point()
logg #shows linear regression model of log data

#add the log values to the original dataframe
log_hr <- c(log(d$HomeRange_km2))
log_fbm <- c(log(d$Body_mass_female_mean))
summary(log_hr)
summary(log_fbm)
head(d)
d <- cbind(d, log_hr, log_fbm)
d <-  d[!is.na(d$log_hr),]
d <-  d[!is.na(d$log_fbm),]
head(d) #d is now the dataframe of original data and the log of home range and log of body masss

summary(d)
d <- as.data.frame(cbind(log_hr, log_fbm))
head(d)
d <-  d[!is.na(log_hr),]
head(d)
nrow(d)
length(log_fbm) #both are 213 values long
length(log_hr)

model <- lm(log_hr~log_fbm, data=d)
model
summary(model)
confint(model, level = 0.95)
#Beta coefficients:
#intercept = -9.441
#slope = 1.036
#there is a significant relationship between the log of the two variables
#standard errors: 0.67293, 0.08488
```

##bootstrapping
```{r}

library(sciplot)
 #Bootstrapping
  s <- NULL #sets s as an emply variable
  m <- rep(0,1000)
  for(i in 1:1000){ #tells the bootstrap to sample 1000 times
  samp <- d[sample(nrow(d), replace=TRUE),] 
  m <- lm(log_hr~log_fbm, samp) #fits bootstrapped data into a linear model
  s <- rbind(s, coef(m)) #creats data fraome of the bootstrapepd coefficients
  }

head(s)
sd(s[,1]) #returns standard error for the y-intercept 
#0.6127816
sd(s[,2]) #returns the standard error the the slope
#0.07816466
quantile(s[,1], c(0.025, 0.975), na.rm = TRUE) #sets the confidence intervals
# -10.78, -8.322
quantile(s[,2], c(0.025, 0.975), na.rm = TRUE) #sets the confidence intervals
# 0.896, 1.2068
```

##The standard errors for the bootstrapped data are slightly lower than the original standard errors from the linear model. The confidence intervals are also similar, to the original model. The 95%Cis for the bootsrapped are slightly lower, but generally comprable I would say.